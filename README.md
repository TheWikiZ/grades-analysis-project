# grades-analysis-project
## About
This is our group's Mini Project for SC1015/CE1115/CZ1115 - Introduction to Data Science and Artificial Intelligence, which focuses on grades extracted from [student's performance on Kaggle](https://www.kaggle.com/datasets/whenamancodes/student-performance/data). For detailed walkthrough, please view the source code in order from:
1. [Data Cleaning](https://github.com/TheWikiZ/grades-analysis-project/blob/main/Data%20Cleaning%20.ipynb)
2. [Data Visualisation](https://github.com/TheWikiZ/grades-analysis-project/blob/main/Data-Visualisation.ipynb)
3. [Data Splitting](https://github.com/TheWikiZ/grades-analysis-project/blob/main/Data-Splitting.ipynb)
4. [Initial Model Observation](https://github.com/TheWikiZ/grades-analysis-project/blob/main/Initial-Model-Observation.ipynb)
5. [Logistic Regression](https://github.com/TheWikiZ/grades-analysis-project/blob/main/Logistic-Regression.ipynb)
6. [Random Forest](https://github.com/TheWikiZ/grades-analysis-project/blob/main/Random-Forest.ipynb)
7. [Gradient Boosting](https://github.com/TheWikiZ/grades-analysis-project/blob/main/Gradient-Boosting.ipynb)
8. [XGBoost](https://github.com/TheWikiZ/grades-analysis-project/blob/main/XGB-Model.ipynb)

   
## Contributors
- @TheWikiZ
- @KWENN13
- @Velosion

## Problem Definition
- Are we able to predict the grades of a student based on the different circumstances?
- If so, which model was the best in predicting the grades?

## Models Used
  1. Logistic Regression
  2. Random Forest
  3. Gradient Boosting
  4. XGBoost

## Conclusion 
- Since the 'Average' class results were the most, the models were able to predict the results very well for those but not so much for the 'High' and 'Low' classes as there were lesser data for those 2 classes.
- Different models had their own different factors which they thought were the most important.
- **We did manage to predict the results based on one’s circumstance, but this should not be taken seriously as we believe that one’s success originates from their own hard work.**


## What we learned
- Logistic Regression
- RandomForest Classifier
- Gradient Boosting
- XGBoost
- Concepts about Precision, Recall, and F1 Score
- Concepts about Macro & Weighted Average, Support

## References
1. <https://www.kaggle.com/datasets/whenamancodes/student-performance/data>
2. <https://www.kaggle.com/code/sharonyaroshetsky/annual-grades-average-5-level-classification>
3. <https://www.kaggle.com/code/archit9406/student-performance-analysis>
4. <https://archive.ics.uci.edu/dataset/320/student+performance>
5. <https://towardsdatascience.com/a-brief-introduction-to-xgboost-3eaee2e3e5d6#:~:text=XGBoost%20vs%20Gradient%20Boosting,which%20improves%20model%20generalization%20capabilities.>
